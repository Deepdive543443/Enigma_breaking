\chapter{Conclusions and Discussion}
\section{Results Analysis}
Our experiment includes two different topics. In the first experiment, our work was focused on find out which neural network architectures would have a good performance on the attacking task. We tried multiple different networks abilities on fitting to a simple task of attack. The attack of Enigma is a time series analytics task, so we chose LSTM as a part of network because it is the start-of-the-art model in time series analytics. We also used Transformer components because of their proven abilities on Large Language Model and other natural language processing task. The results show that LSTM is the indispensable part of our task. Networks without LSTM layers have overall lower performance. Also, a network with balanced number of LSTM layers and Transformer Encoder layers has overall an amazing performance on reaching more than 99 of accuracy on recovering the initial states. Networks with more channels (512 dimensions) have better convergence speed. In this part of experience, we determine that a network that combines LSTM and Transformer components together could achieve much higher accuracy for analysing the cipher. And we used this network as the baseline of further experiment with more complexity.

The second part of experiment aims to find the limitations of networks in modelling more variables. We used all Enigma’s settings as variable and tested how many of them could be recovered by the network just by looking at the ciphertext and plaintext inputs. The result was as expected. Since some of the variables bring large complexity to the cipher, network has failed it mission on attacking all the setting of Enigma, but for the cipher with less complexity, it did achieve acceptable performance. The overall result shows that neural networks have some ability to analyse substitution ciphers with lower complexity. 

\section{Further works}
Three potential approaches could be applied to improve the performance from current results. The first one is the dataset we used for training. We generate all our training data randomly which is different from real life data. In the natural language data from real-life, the appearance of each character is not balance. According to \cite{heise1965semantic}, character “T” has higher apprearance becasue of it is adopted by lots of high frequency words like “The”. sommervoll et al \cite{sommervoll2021genetic} state that in ciphertext, each character has balanced appearance which is close to a randomly generated string. It means that our plaintext and ciphertext share a similar probabilistic distribution. This might already cause some negative impact on our training results. One of the improvements we could made is using plaintext from real-life environment to train our model. This could be a large natural language dataset, or other content scraped from internet.

A second improvement we could made is in the architecture. The network we used in this project is a simple combination of LSTM and Transformer Encoder layers, which is not the state of the art in the time series analysis field. As we described in the literature review part, there are more Transformer and LSTM hybrid model specific design for time series analytic scenes. We expected these networks would also be a good fit on our task on cryptanalysis. 

The third is parameter searching. Due to the limitation of computation resources, we didn’t perform parameters searching. The future works should focus on observation the performance of time series analytic on our current task, perform grid search for better parameters setting for training. 

Enigma represents the peak of classical ciphers. At the end of the paper “Learning the Enigma with recurrent neural networks”, Greydanus \cite{greydanus2017learning} mentioned his interest on applying deep learning technique for modern cipher’s cryptanalysis, including the well-known RSA algorithm. We expected deep learning cryptanalysis for modern cipher would be a very challenging task. One of the most significant reasons is the complexity of modern encryption algorithm. Modern ciphers have their encryption implemented on the byte level which leads to extremely high complexity. For example, AES-256 has complexity at \(2^\text{254.4}\) and it could take years for a cluster of modern computers to break. A three rotors Enigma has around 1.5e14 of complexity, but our current network still has a hard time to attack and analyse over part of the ciphertext. Another assumption we made is that, classical ciphers like Enigma used substitution for Encryption, this is process that could be reversed directly, but in modern cipher, especially asymmetric cipher, this process is a lot more complicated. Overall, modern ciphers are designed with awareness of potential security issues. This could include the impact from machine learning. The current research regarding Cybersecurity and AI were focused on defensive purpose. This includes the detection of threads, malicious behaviour in the network environment or machine learning based authentication. Modern encryption algorithms were designed and engineered to have high complexity and reliability, and these features have been proved since they have been used in industrial and other area that require highly security. We believe that it is nearly impossible to attack modern cipher using deep learning approach. But like the Quantum computing, machine learning might be a potential area that could make big impact on cybersecurity.

\section{Conclusion}
In this project, we first review the background of classical ciphers, modern machine learning, and the researches that focus on the attacking classical cipher by modern machine learning algorithms (deep learning, reinforcement learning, etc). Then we followed the research of Greydanus \cite{greydanus2017learning} and made our own experiments on using modern Natural Language Processing and time series analytics models on analysing the key phrase (initial rotor state) of Enigma. The result shows that a LSTM and Transformer hybrid network could learns to recover the keys using ciphertexts and plaintexts. We also attempted to add more variables (rotors type, reflector type, plugboard setting, ring settings) to extended the complexity of Enigma. It turns out the network would have more difficulty to fit with the data when the complexity of Enigma grows. And the network still far from given the entire Enigma machine. The results show that machine learning ability on analysis high complexity cipher is still limited. 

At the end of the project, we discussed about the probability of using machine learning to attack the modern encryption algorithms, include some well-known asymmetric and symmetric algorithms like RSA and AES. The results of previous experiments on Enigma, we assume it is nearly impossible because our network still has challenges to deal with the classical ciphers with high complexity, and modern cipher and encryption algorithm all have complexity that greatly exceed that of classical cipher. Currently, application of AI in security is mostly in defensive usage. But like Quantum computing, we still need to treat machine learning carefully because of it have potential to make huge impact on cryptographic and cybersecurity. We believe that machine learning would be able to help cryptographer to target the vulnerability in cipher in the future.
